{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielP53/Labs-SP1/blob/master/Lab09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpwTxfsGN5Qj",
        "colab_type": "text"
      },
      "source": [
        "# **Copyright Information**\n",
        "\n",
        "####Copyright David Gündisch\n",
        "#####El código original de este tutorial pertenece a David Gündisch. Este código fue modificado agregando comentarios, explicaciones e imágenes para ser una práctica interactiva educativa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tHDd2Aye8zf",
        "colab_type": "text"
      },
      "source": [
        "### **Laboratorio 8: Redes Generativas Adversariales**\n",
        "En este laboratorio crearemos una Red Generativa Adversarial para generar nuevas imágenes de dígitos, similares a los dígitos del dataset MNIST.\n",
        "\n",
        "**Nota: recuerde activar la conexión entre Google Colab y Google Drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt4Byq00Z0Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only works in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJf_oqFYZ4Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0V7Eedu7Yt",
        "colab_type": "text"
      },
      "source": [
        "Las Redes Generativas Adversariales son una forma de generar un modelo utilizando dos redes neurales que compiten una con otra.\n",
        "\n",
        "El **generador** convierte ruido en una imitación de los datos intentando engañar al discriminador.\n",
        "\n",
        "EL **discriminador** intenta identificar los datos reales de los datos falsos creados por el generador.\n",
        "\n",
        "El generador recibe como entrada ruido aleatorio. Solamente el discriminador tiene acceso a los datos de entrenamiento para propósitos de clasificación.\n",
        "El generador mejora el resultado de su salida solamente utilizando la retroalimentación obtenida del discriminador.\n",
        "\n",
        "![texto alternativo](https://www.pngitem.com/pimgs/m/96-965914_a-short-introduction-to-generative-adversarial-networks-generative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzMXgUhbu0V2",
        "colab_type": "text"
      },
      "source": [
        "La clase GAN() contiene los métodos necesarios para construir nuestra red y entrenarla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Od-PHQZ6Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        \n",
        "        '''TODO: Especifique la dimensión del espacio latente e.g. 100'''\n",
        "        self.latent_dim = 100\n",
        "        '''TODO: Especifique el optimizador que minimiza la función de perdida'''\n",
        "        optimizer = Adam(0.001,0.9)\n",
        "        \n",
        "        ### Discriminador de la GAN\n",
        "        self.discriminator = self.build_discriminator()\n",
        "\n",
        "        '''TODO: Coloque la función de perdida y la métrica correcta para el discriminador, ponga atención al trabajo\n",
        "        que está realizando el discriminador. e.g. clasificación multiclase, clasificación binaria, regresión.'''\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "        \n",
        "        ### Generador de la GAN\n",
        "        self.generator = self.build_generator()\n",
        "        '''TODO: z es la entrada hacia el generador (vector 1D de ruido). Indique la dimensión de entrada correcta para z.'''\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        \n",
        "        '''TODO: el generador recibe ruido como entrada y devuelve una imagen'''\n",
        "        img = self.generator(z)\n",
        "        \n",
        "        '''TODO: No deseamos que la gradiente se propague hacia el discriminador, solamente deseamos propagar las gradientes\n",
        "        hacia el generador'''\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        '''TODO: El discriminador recibe la imagen creada por el generador y nos indica si la imagen es verdadera o falsa'''\n",
        "        validity = self.discriminator(img)\n",
        "        \n",
        "        '''TODO': Creamos un modelo con el generador y el discriminador unidos, especifique la entrada para el modelo completo\n",
        "        y la salida correspondiente. '''\n",
        "        self.combined = Model(inputs= z, outputs= validity)\n",
        "        \n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        ### Este método construye la parte del generador de la GAN\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        \n",
        "        '''TODO: Agregue una capa totalmente conectada con una función de activación LeakyReLU y BatchNormalization.\n",
        "        Agregue una nueva capa totalmente conectada adicional con función de activación LeakyReLU y BatchNormalization\n",
        "        Recuerde que iniciamos nuestra entrada al generador con la dimensión del espacio latente e.g. 100, debemos ir\n",
        "        aumentando las dimensiones de forma similar a la estructura utilizada en la sección 'decoder' en un autoencoder.\n",
        "        Sabiendo esto debe indicar los valores correctos para la cantidad de salidas en cada capa densa.'''\n",
        "    \n",
        "        '''TODO: capa 1'''\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        '''TODO: capa 2'''\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        \n",
        "        ### La penúltima capa del generador es un vector 1D con dimensión ancho_imagen x alto_imagen e.g.784\n",
        "        \n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        \n",
        "        '''TODO: Una imagen de un dígito de MNIST es un objeto 2D, nuestra capa anterior es 1D, debemos pasar de 1D (784,) \n",
        "        a 2D (28,28) utilizando Reshape(new_size)'''\n",
        "        model.add(Reshape(self.img_shape))\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "        \n",
        "        '''TODO': La función build_generator() devuelve solamente la red neural que corresponde a la parte del generador.\n",
        "        Recuerde lo que recibe el generador como entrada y que entrega en la salida.'''\n",
        "        return Model(inputs=noise, outputs=img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "    \n",
        "        model = Sequential()\n",
        "        ### El discriminador es un clasificador, este clasificador no utiliza CNN, es un clasificador con una FCN, su entrada es un vector 1D\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        \n",
        "        '''TODO: agregue dos capas densas con función de activación LeakyRelu'''\n",
        "        '''TODO: capa 1'''\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        '''TODO: capa 2'''\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        '''TODO: el discriminador es un clasificador. ¿Es un clasificador binario o debe clasificar mas de 2 clases?'''\n",
        "        \n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "        \n",
        "        '''TODO': La función build_discriminator() devuelve solamente la red neural que corresponde a la parte del discriminador.\n",
        "           Recuerde lo que recibe el discriminador como entrada y que entrega en la salida.'''\n",
        "        return Model(inputs=img, outputs=validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        ### Método de entrenamiento de la GAN\n",
        "        \n",
        "        ### Se carga el dataset MNIST\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "        \n",
        "        ### Escalamos el valor de los pixels al rango (-1,1)\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        \n",
        "        ### El dataset contiene 60k imágenes de 28x28 pixels, los caracteres son blanco y negro, por lo tanto usualmente \n",
        "        ### el shape del dataset no incluye canales. Expandimos nuestro dataset para indicar que solo tenemos 1 canal.\n",
        "        ### dataset shape  = (60000, 28, 28) -> (60000, 28, 28, 1)\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        \n",
        "        ### Etiquetas (labels) para enviar al discriminador para las imágenes verdaderas y falsas. \n",
        "        ### imagen verdadera = 1 , imagen falsa = 0\n",
        "        ### valid.shape = (132, 1)   \n",
        "        ### fake.shape(132,1)\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        ### Proceso iterativo de entrenamiento del modelo\n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            ### Se carga un batch para entrenamiento\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "            \n",
        "            \n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            '''TODO: train_on_batch ejecuta una única actualización de gradiente con un batch de datos\n",
        "                \n",
        "               Entrenamos el generador en el modelo combinado, utilizando la retroalimentación\n",
        "               que nos da el discriminador.         \n",
        "               Realizamos foward_pass a través del modelo combinado generador + discriminador. \n",
        "               pero en backward_pass solo propagamos la gradiente hacia el generador, logramos esto \n",
        "               porque recuerde que anteriormente indicamos que los pesos del discriminador \n",
        "               serán no entrenables en el modelo combinado.\n",
        "               Indicaremos que las etiquetas(labels) para las imágenes creadas por el generador \n",
        "               seran verdaderas e.g 1. (porque deseamos hacer pasarlas por reales)\n",
        "             \n",
        "               En un una iteración el discriminador está entrenado para distinguir \n",
        "               imágenes verdaderas y falsas.\n",
        "               El generador a partir de ruido genera una imagen. En este momento\n",
        "               no sabemos si esta imagen es lo suficiente buena para pasar por real, pero queremos hacerla \n",
        "               pasar por real, por lo tanto como etiqueta indicaremos que esta imagen es verdadera e.g 1.\n",
        "               Deseamos que el generador se acerque lo mas posible a generar imágenes reales.\n",
        "               Posteriormente esta imagen pasa al discriminador a través del modelo combinado y el \n",
        "               discriminador podrá decirnos si es verdadera o falsa.\n",
        "               Utilizando esa información que nos da el discriminador, modificaremos los pesos del generador \n",
        "               para que la próxima iteración podamos acercarnos más a engañar al discriminador.'''\n",
        "            \n",
        "\n",
        "            g_loss = self.combined.train_on_batch(x=noise, y=valid)\n",
        "\n",
        "            '''TODO: Posteriormente de haber realizado una actualización de los pesos del generador, generaremos un batch de\n",
        "               imágenes (falsas) a partir de ruido.''' \n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            \n",
        "            '''TODO: Entrenamos al discriminador para que aprenda a clasificar \n",
        "               correctamente imágenes verdaderas. Para esto debemos enviarle ejemplos\n",
        "               de imágenes reales. Las imágenes verdaderas son las imágenes\n",
        "               que se obtienen del dataset MNIST. Debemos indicarle al discriminador \n",
        "               que estas imágenes son verdaderas, esto lo hacemos asignando a cada imagen una etiqueta\n",
        "               verdadera e.g 1'''\n",
        "            d_loss_real = self.discriminator.train_on_batch(x=imgs, y=valid)\n",
        "\n",
        "            '''TODO: Entrenamos al discriminador para que aprenda a identificar\n",
        "               correctamente imágenes falsas. Para esto debemos enviarle ejemplos\n",
        "               de imágenes falsas. Las imágenes falsas son las imágenes\n",
        "               que se obtienen del generador. Debemos indicarle al discriminador \n",
        "               que estas imágenes son falsas, esto lo hacemos asignando a cada imagen una etiqueta\n",
        "               falsa e.g 0'''\n",
        "            d_loss_fake = self.discriminator.train_on_batch(x=gen_imgs, y=fake)\n",
        "\n",
        "            ### valor de perdida del discriminador\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            \n",
        "            if epoch % 50 == 0:\n",
        "                print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        ### Función que crea un archivo .png con ejemplos de imágenes nuevas generadas por\n",
        "        ### el generador\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r68GIhOuUiHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faff9161-462e-4c42-f236-d8b9813749e0"
      },
      "source": [
        "### Al finalizar el entrenamiento vea las imágenes resultantes \n",
        "### creadas por el generador a través de cada iteración en su directorio de Google Drive.\n",
        "\n",
        "gan = GAN()\n",
        "gan.train(epochs=100000, batch_size=132, sample_interval=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "0 [D loss: 0.736458, acc.: 40.53%] [G loss: 0.746874]\n",
            "50 [D loss: 0.205156, acc.: 94.32%] [G loss: 14.654696]\n",
            "100 [D loss: 0.004149, acc.: 100.00%] [G loss: 14.551773]\n",
            "150 [D loss: 0.044249, acc.: 98.86%] [G loss: 10.652032]\n",
            "200 [D loss: 0.004823, acc.: 100.00%] [G loss: 7.892367]\n",
            "250 [D loss: 0.032886, acc.: 99.24%] [G loss: 16.791779]\n",
            "300 [D loss: 0.049216, acc.: 98.48%] [G loss: 7.869254]\n",
            "350 [D loss: 0.007075, acc.: 100.00%] [G loss: 9.666026]\n",
            "400 [D loss: 0.001541, acc.: 100.00%] [G loss: 12.858888]\n",
            "450 [D loss: 0.029873, acc.: 99.62%] [G loss: 6.215681]\n",
            "500 [D loss: 0.037980, acc.: 99.62%] [G loss: 5.393120]\n",
            "550 [D loss: 0.011040, acc.: 100.00%] [G loss: 6.307223]\n",
            "600 [D loss: 0.003401, acc.: 100.00%] [G loss: 6.535523]\n",
            "650 [D loss: 0.006173, acc.: 100.00%] [G loss: 7.573958]\n",
            "700 [D loss: 0.015854, acc.: 99.62%] [G loss: 7.136625]\n",
            "750 [D loss: 0.012947, acc.: 100.00%] [G loss: 9.843990]\n",
            "800 [D loss: 0.016499, acc.: 100.00%] [G loss: 6.587492]\n",
            "850 [D loss: 0.038290, acc.: 99.62%] [G loss: 6.802112]\n",
            "900 [D loss: 0.031641, acc.: 99.24%] [G loss: 6.491177]\n",
            "950 [D loss: 0.008507, acc.: 100.00%] [G loss: 6.738743]\n",
            "1000 [D loss: 0.004787, acc.: 100.00%] [G loss: 7.150740]\n",
            "1050 [D loss: 0.003347, acc.: 100.00%] [G loss: 8.390073]\n",
            "1100 [D loss: 0.002281, acc.: 100.00%] [G loss: 10.821215]\n",
            "1150 [D loss: 0.006992, acc.: 100.00%] [G loss: 8.117953]\n",
            "1200 [D loss: 0.039292, acc.: 98.86%] [G loss: 6.519200]\n",
            "1250 [D loss: 0.046548, acc.: 98.48%] [G loss: 5.895530]\n",
            "1300 [D loss: 0.007582, acc.: 100.00%] [G loss: 7.827971]\n",
            "1350 [D loss: 0.005405, acc.: 100.00%] [G loss: 9.672290]\n",
            "1400 [D loss: 0.003500, acc.: 100.00%] [G loss: 16.080662]\n",
            "1450 [D loss: 0.015302, acc.: 99.62%] [G loss: 6.916050]\n",
            "1500 [D loss: 0.002291, acc.: 100.00%] [G loss: 11.199841]\n",
            "1550 [D loss: 0.006500, acc.: 100.00%] [G loss: 8.630278]\n",
            "1600 [D loss: 0.006487, acc.: 99.62%] [G loss: 9.724253]\n",
            "1650 [D loss: 0.010417, acc.: 100.00%] [G loss: 7.156073]\n",
            "1700 [D loss: 0.024337, acc.: 99.62%] [G loss: 11.985918]\n",
            "1750 [D loss: 0.015945, acc.: 99.62%] [G loss: 19.230272]\n",
            "1800 [D loss: 0.004280, acc.: 100.00%] [G loss: 7.429346]\n",
            "1850 [D loss: 0.000296, acc.: 100.00%] [G loss: 9.142369]\n",
            "1900 [D loss: 0.018000, acc.: 99.62%] [G loss: 9.654191]\n",
            "1950 [D loss: 0.026712, acc.: 99.24%] [G loss: 7.014046]\n",
            "2000 [D loss: 0.019648, acc.: 99.62%] [G loss: 7.373268]\n",
            "2050 [D loss: 0.009297, acc.: 100.00%] [G loss: 8.901568]\n",
            "2100 [D loss: 0.013900, acc.: 100.00%] [G loss: 6.497877]\n",
            "2150 [D loss: 0.008721, acc.: 100.00%] [G loss: 10.658505]\n",
            "2200 [D loss: 0.033495, acc.: 98.48%] [G loss: 30.122101]\n",
            "2250 [D loss: 0.018925, acc.: 99.62%] [G loss: 7.515504]\n",
            "2300 [D loss: 0.007162, acc.: 100.00%] [G loss: 6.395796]\n",
            "2350 [D loss: 0.002095, acc.: 100.00%] [G loss: 7.313961]\n",
            "2400 [D loss: 0.004146, acc.: 100.00%] [G loss: 8.562189]\n",
            "2450 [D loss: 6.783000, acc.: 50.00%] [G loss: 0.031261]\n",
            "2500 [D loss: 0.051951, acc.: 98.48%] [G loss: 5.426063]\n",
            "2550 [D loss: 0.012816, acc.: 99.24%] [G loss: 7.001149]\n",
            "2600 [D loss: 0.044610, acc.: 98.11%] [G loss: 9.917508]\n",
            "2650 [D loss: 0.042620, acc.: 100.00%] [G loss: 4.777363]\n",
            "2700 [D loss: 0.024076, acc.: 98.86%] [G loss: 7.677003]\n",
            "2750 [D loss: 0.022324, acc.: 99.62%] [G loss: 7.407015]\n",
            "2800 [D loss: 0.010436, acc.: 100.00%] [G loss: 7.174399]\n",
            "2850 [D loss: 0.036892, acc.: 99.62%] [G loss: 5.807206]\n",
            "2900 [D loss: 0.015633, acc.: 100.00%] [G loss: 6.438863]\n",
            "2950 [D loss: 0.017650, acc.: 100.00%] [G loss: 6.135414]\n",
            "3000 [D loss: 0.007792, acc.: 100.00%] [G loss: 8.668576]\n",
            "3050 [D loss: 0.008932, acc.: 100.00%] [G loss: 7.775712]\n",
            "3100 [D loss: 0.004987, acc.: 100.00%] [G loss: 7.303458]\n",
            "3150 [D loss: 0.001898, acc.: 100.00%] [G loss: 17.002501]\n",
            "3200 [D loss: 0.060618, acc.: 96.97%] [G loss: 7.624489]\n",
            "3250 [D loss: 0.004111, acc.: 100.00%] [G loss: 7.649428]\n",
            "3300 [D loss: 0.037733, acc.: 98.86%] [G loss: 12.302361]\n",
            "3350 [D loss: 0.023734, acc.: 98.86%] [G loss: 6.812045]\n",
            "3400 [D loss: 0.062857, acc.: 98.11%] [G loss: 5.512273]\n",
            "3450 [D loss: 0.007193, acc.: 100.00%] [G loss: 7.451156]\n",
            "3500 [D loss: 0.002889, acc.: 100.00%] [G loss: 7.755568]\n",
            "3550 [D loss: 0.006218, acc.: 100.00%] [G loss: 8.323707]\n",
            "3600 [D loss: 0.008852, acc.: 100.00%] [G loss: 7.467063]\n",
            "3650 [D loss: 0.014312, acc.: 99.62%] [G loss: 10.304516]\n",
            "3700 [D loss: 0.029404, acc.: 99.62%] [G loss: 6.259890]\n",
            "3750 [D loss: 0.018193, acc.: 100.00%] [G loss: 5.658251]\n",
            "3800 [D loss: 0.004136, acc.: 100.00%] [G loss: 7.049045]\n",
            "3850 [D loss: 0.014543, acc.: 99.62%] [G loss: 12.181308]\n",
            "3900 [D loss: 0.029307, acc.: 99.62%] [G loss: 4.799064]\n",
            "3950 [D loss: 0.014044, acc.: 100.00%] [G loss: 6.191385]\n",
            "4000 [D loss: 0.005562, acc.: 100.00%] [G loss: 7.159786]\n",
            "4050 [D loss: 0.006837, acc.: 100.00%] [G loss: 7.064610]\n",
            "4100 [D loss: 0.060051, acc.: 98.11%] [G loss: 7.581123]\n",
            "4150 [D loss: 0.027475, acc.: 99.62%] [G loss: 5.148121]\n",
            "4200 [D loss: 0.026757, acc.: 99.62%] [G loss: 8.035540]\n",
            "4250 [D loss: 0.008886, acc.: 100.00%] [G loss: 8.326971]\n",
            "4300 [D loss: 0.048625, acc.: 99.24%] [G loss: 7.067603]\n",
            "4350 [D loss: 0.005512, acc.: 100.00%] [G loss: 7.484104]\n",
            "4400 [D loss: 0.018323, acc.: 100.00%] [G loss: 6.116921]\n",
            "4450 [D loss: 0.010086, acc.: 100.00%] [G loss: 7.209600]\n",
            "4500 [D loss: 0.003987, acc.: 100.00%] [G loss: 6.966550]\n",
            "4550 [D loss: 0.018183, acc.: 99.62%] [G loss: 9.048009]\n",
            "4600 [D loss: 0.080131, acc.: 98.11%] [G loss: 5.730689]\n",
            "4650 [D loss: 0.012909, acc.: 100.00%] [G loss: 6.744277]\n",
            "4700 [D loss: 0.015663, acc.: 100.00%] [G loss: 5.964083]\n",
            "4750 [D loss: 0.008183, acc.: 100.00%] [G loss: 8.775397]\n",
            "4800 [D loss: 0.012120, acc.: 100.00%] [G loss: 8.514894]\n",
            "4850 [D loss: 0.008996, acc.: 99.62%] [G loss: 6.951482]\n",
            "4900 [D loss: 0.004541, acc.: 100.00%] [G loss: 6.424885]\n",
            "4950 [D loss: 0.003226, acc.: 100.00%] [G loss: 33.299053]\n",
            "5000 [D loss: 0.003435, acc.: 100.00%] [G loss: 9.473402]\n",
            "5050 [D loss: 0.023062, acc.: 99.24%] [G loss: 8.712087]\n",
            "5100 [D loss: 0.002585, acc.: 100.00%] [G loss: 9.917353]\n",
            "5150 [D loss: 0.173547, acc.: 95.83%] [G loss: 29.788946]\n",
            "5200 [D loss: 0.022013, acc.: 99.24%] [G loss: 9.240295]\n",
            "5250 [D loss: 0.025298, acc.: 99.24%] [G loss: 6.599108]\n",
            "5300 [D loss: 0.022021, acc.: 99.62%] [G loss: 5.779447]\n",
            "5350 [D loss: 0.004463, acc.: 100.00%] [G loss: 6.968701]\n",
            "5400 [D loss: 0.007232, acc.: 100.00%] [G loss: 6.903083]\n",
            "5450 [D loss: 0.065719, acc.: 96.59%] [G loss: 12.249403]\n",
            "5500 [D loss: 0.031894, acc.: 99.24%] [G loss: 5.653392]\n",
            "5550 [D loss: 0.008993, acc.: 100.00%] [G loss: 5.933862]\n",
            "5600 [D loss: 0.010432, acc.: 99.62%] [G loss: 7.779270]\n",
            "5650 [D loss: 0.033287, acc.: 99.62%] [G loss: 4.496680]\n",
            "5700 [D loss: 0.014263, acc.: 100.00%] [G loss: 5.433767]\n",
            "5750 [D loss: 0.013215, acc.: 100.00%] [G loss: 5.681832]\n",
            "5800 [D loss: 0.010321, acc.: 100.00%] [G loss: 5.946297]\n",
            "5850 [D loss: 0.025365, acc.: 99.62%] [G loss: 5.020418]\n",
            "5900 [D loss: 0.022112, acc.: 100.00%] [G loss: 4.738389]\n",
            "5950 [D loss: 0.009402, acc.: 100.00%] [G loss: 7.085041]\n",
            "6000 [D loss: 0.002972, acc.: 100.00%] [G loss: 6.303327]\n",
            "6050 [D loss: 0.002249, acc.: 100.00%] [G loss: 7.268475]\n",
            "6100 [D loss: 0.011953, acc.: 99.62%] [G loss: 9.100524]\n",
            "6150 [D loss: 0.025535, acc.: 99.24%] [G loss: 6.305498]\n",
            "6200 [D loss: 0.011914, acc.: 100.00%] [G loss: 6.202184]\n",
            "6250 [D loss: 0.011193, acc.: 100.00%] [G loss: 8.541643]\n",
            "6300 [D loss: 0.007710, acc.: 100.00%] [G loss: 6.203887]\n",
            "6350 [D loss: 0.005905, acc.: 100.00%] [G loss: 8.212502]\n",
            "6400 [D loss: 0.115630, acc.: 95.83%] [G loss: 15.230524]\n",
            "6450 [D loss: 0.032324, acc.: 99.24%] [G loss: 4.940096]\n",
            "6500 [D loss: 0.033683, acc.: 99.24%] [G loss: 6.298528]\n",
            "6550 [D loss: 0.011698, acc.: 100.00%] [G loss: 6.369663]\n",
            "6600 [D loss: 0.005202, acc.: 100.00%] [G loss: 8.239755]\n",
            "6650 [D loss: 0.005062, acc.: 100.00%] [G loss: 8.691118]\n",
            "6700 [D loss: 0.005909, acc.: 100.00%] [G loss: 8.719804]\n",
            "6750 [D loss: 0.020576, acc.: 99.24%] [G loss: 10.044185]\n",
            "6800 [D loss: 0.000936, acc.: 100.00%] [G loss: 14.668896]\n",
            "6850 [D loss: 0.001536, acc.: 100.00%] [G loss: 19.815052]\n",
            "6900 [D loss: 0.030179, acc.: 98.86%] [G loss: 7.210739]\n",
            "6950 [D loss: 5.261072, acc.: 53.03%] [G loss: 0.558923]\n",
            "7000 [D loss: 0.053217, acc.: 99.24%] [G loss: 4.809298]\n",
            "7050 [D loss: 0.344882, acc.: 90.53%] [G loss: 24.294188]\n",
            "7100 [D loss: 0.025611, acc.: 100.00%] [G loss: 5.822335]\n",
            "7150 [D loss: 0.019081, acc.: 100.00%] [G loss: 4.782158]\n",
            "7200 [D loss: 0.007793, acc.: 100.00%] [G loss: 7.090554]\n",
            "7250 [D loss: 0.038682, acc.: 98.48%] [G loss: 8.768932]\n",
            "7300 [D loss: 0.008750, acc.: 100.00%] [G loss: 7.453488]\n",
            "7350 [D loss: 0.015432, acc.: 99.62%] [G loss: 9.218472]\n",
            "7400 [D loss: 0.036300, acc.: 98.11%] [G loss: 14.924354]\n",
            "7450 [D loss: 0.024791, acc.: 100.00%] [G loss: 4.637777]\n",
            "7500 [D loss: 0.027568, acc.: 99.62%] [G loss: 5.603338]\n",
            "7550 [D loss: 0.014242, acc.: 100.00%] [G loss: 6.230542]\n",
            "7600 [D loss: 0.024878, acc.: 99.24%] [G loss: 7.745211]\n",
            "7650 [D loss: 0.013310, acc.: 100.00%] [G loss: 6.576387]\n",
            "7700 [D loss: 0.004811, acc.: 100.00%] [G loss: 9.717106]\n",
            "7750 [D loss: 0.011094, acc.: 100.00%] [G loss: 7.825980]\n",
            "7800 [D loss: 0.017676, acc.: 99.24%] [G loss: 8.063150]\n",
            "7850 [D loss: 0.005665, acc.: 100.00%] [G loss: 8.554821]\n",
            "7900 [D loss: 0.033145, acc.: 99.24%] [G loss: 9.503286]\n",
            "7950 [D loss: 0.028688, acc.: 100.00%] [G loss: 4.795400]\n",
            "8000 [D loss: 0.027164, acc.: 99.62%] [G loss: 6.417824]\n",
            "8050 [D loss: 0.003704, acc.: 100.00%] [G loss: 6.656009]\n",
            "8100 [D loss: 0.055101, acc.: 98.48%] [G loss: 4.817376]\n",
            "8150 [D loss: 0.027505, acc.: 99.62%] [G loss: 6.100830]\n",
            "8200 [D loss: 0.014493, acc.: 99.62%] [G loss: 4.840632]\n",
            "8250 [D loss: 0.034059, acc.: 99.24%] [G loss: 6.085116]\n",
            "8300 [D loss: 0.006770, acc.: 100.00%] [G loss: 6.585350]\n",
            "8350 [D loss: 0.003074, acc.: 100.00%] [G loss: 9.371546]\n",
            "8400 [D loss: 0.003380, acc.: 100.00%] [G loss: 8.837878]\n",
            "8450 [D loss: 0.004091, acc.: 100.00%] [G loss: 7.620635]\n",
            "8500 [D loss: 0.039798, acc.: 99.24%] [G loss: 9.291241]\n",
            "8550 [D loss: 0.042376, acc.: 98.86%] [G loss: 6.118379]\n",
            "8600 [D loss: 0.009380, acc.: 100.00%] [G loss: 6.568594]\n",
            "8650 [D loss: 0.012999, acc.: 100.00%] [G loss: 8.533539]\n",
            "8700 [D loss: 0.052384, acc.: 98.48%] [G loss: 6.085536]\n",
            "8750 [D loss: 0.024266, acc.: 99.62%] [G loss: 6.635018]\n",
            "8800 [D loss: 0.026470, acc.: 99.62%] [G loss: 5.667626]\n",
            "8850 [D loss: 0.010484, acc.: 100.00%] [G loss: 6.493612]\n",
            "8900 [D loss: 0.018772, acc.: 100.00%] [G loss: 7.696549]\n",
            "8950 [D loss: 0.067873, acc.: 98.48%] [G loss: 5.972465]\n",
            "9000 [D loss: 0.024928, acc.: 99.62%] [G loss: 7.318744]\n",
            "9050 [D loss: 0.009332, acc.: 100.00%] [G loss: 6.094224]\n",
            "9100 [D loss: 0.003093, acc.: 100.00%] [G loss: 8.118618]\n",
            "9150 [D loss: 2.256356, acc.: 47.73%] [G loss: 27.548048]\n",
            "9200 [D loss: 0.011990, acc.: 100.00%] [G loss: 7.218401]\n",
            "9250 [D loss: 0.049034, acc.: 98.48%] [G loss: 8.234789]\n",
            "9300 [D loss: 0.009771, acc.: 99.62%] [G loss: 8.076324]\n",
            "9350 [D loss: 0.006718, acc.: 100.00%] [G loss: 7.185871]\n",
            "9400 [D loss: 0.002474, acc.: 100.00%] [G loss: 9.367938]\n",
            "9450 [D loss: 0.002772, acc.: 100.00%] [G loss: 8.949575]\n",
            "9500 [D loss: 0.001818, acc.: 100.00%] [G loss: 8.079865]\n",
            "9550 [D loss: 0.005696, acc.: 99.62%] [G loss: 13.088886]\n",
            "9600 [D loss: 0.016635, acc.: 99.62%] [G loss: 9.185587]\n",
            "9650 [D loss: 0.027437, acc.: 99.62%] [G loss: 8.791366]\n",
            "9700 [D loss: 0.017748, acc.: 100.00%] [G loss: 9.205857]\n",
            "9750 [D loss: 0.035080, acc.: 98.86%] [G loss: 7.924553]\n",
            "9800 [D loss: 0.015013, acc.: 100.00%] [G loss: 9.275028]\n",
            "9850 [D loss: 0.010015, acc.: 99.62%] [G loss: 9.002841]\n",
            "9900 [D loss: 0.005565, acc.: 100.00%] [G loss: 7.251699]\n",
            "9950 [D loss: 0.017410, acc.: 99.62%] [G loss: 6.866573]\n",
            "10000 [D loss: 0.030228, acc.: 98.48%] [G loss: 6.370334]\n",
            "10050 [D loss: 0.034217, acc.: 99.24%] [G loss: 6.845431]\n",
            "10100 [D loss: 0.014375, acc.: 99.62%] [G loss: 5.885588]\n",
            "10150 [D loss: 0.007337, acc.: 100.00%] [G loss: 6.761401]\n",
            "10200 [D loss: 0.019835, acc.: 99.24%] [G loss: 29.534977]\n",
            "10250 [D loss: 0.002310, acc.: 100.00%] [G loss: 7.880391]\n",
            "10300 [D loss: 0.004501, acc.: 100.00%] [G loss: 11.079113]\n",
            "10350 [D loss: 0.002655, acc.: 100.00%] [G loss: 8.882298]\n",
            "10400 [D loss: 0.000756, acc.: 100.00%] [G loss: 17.997955]\n",
            "10450 [D loss: 0.034175, acc.: 98.86%] [G loss: 6.909946]\n",
            "10500 [D loss: 0.010164, acc.: 99.62%] [G loss: 8.629846]\n",
            "10550 [D loss: 0.003044, acc.: 100.00%] [G loss: 13.295316]\n",
            "10600 [D loss: 0.021132, acc.: 100.00%] [G loss: 8.472868]\n",
            "10650 [D loss: 0.019082, acc.: 99.24%] [G loss: 6.537915]\n",
            "10700 [D loss: 0.008903, acc.: 100.00%] [G loss: 6.861952]\n",
            "10750 [D loss: 0.010804, acc.: 100.00%] [G loss: 7.807546]\n",
            "10800 [D loss: 0.008956, acc.: 100.00%] [G loss: 9.356460]\n",
            "10850 [D loss: 0.007546, acc.: 100.00%] [G loss: 7.610810]\n",
            "10900 [D loss: 0.003193, acc.: 100.00%] [G loss: 11.315729]\n",
            "10950 [D loss: 0.482012, acc.: 87.88%] [G loss: 25.450472]\n",
            "11000 [D loss: 0.028571, acc.: 98.86%] [G loss: 6.113716]\n",
            "11050 [D loss: 0.021595, acc.: 99.24%] [G loss: 7.236560]\n",
            "11100 [D loss: 0.046740, acc.: 98.11%] [G loss: 5.269286]\n",
            "11150 [D loss: 0.010506, acc.: 100.00%] [G loss: 8.977208]\n",
            "11200 [D loss: 0.033313, acc.: 98.48%] [G loss: 9.725400]\n",
            "11250 [D loss: 0.009862, acc.: 100.00%] [G loss: 6.441487]\n",
            "11300 [D loss: 0.042321, acc.: 98.11%] [G loss: 7.792530]\n",
            "11350 [D loss: 0.032354, acc.: 98.86%] [G loss: 6.984681]\n",
            "11400 [D loss: 0.028016, acc.: 99.62%] [G loss: 7.191655]\n",
            "11450 [D loss: 0.051750, acc.: 99.24%] [G loss: 7.449362]\n",
            "11500 [D loss: 0.008581, acc.: 100.00%] [G loss: 7.426416]\n",
            "11550 [D loss: 0.005545, acc.: 100.00%] [G loss: 7.722654]\n",
            "11600 [D loss: 0.002232, acc.: 100.00%] [G loss: 22.376360]\n",
            "11650 [D loss: 0.039082, acc.: 99.62%] [G loss: 8.256972]\n",
            "11700 [D loss: 0.015655, acc.: 99.62%] [G loss: 7.983983]\n",
            "11750 [D loss: 0.013179, acc.: 100.00%] [G loss: 6.758610]\n",
            "11800 [D loss: 0.012424, acc.: 99.62%] [G loss: 6.919803]\n",
            "11850 [D loss: 0.008434, acc.: 100.00%] [G loss: 9.262641]\n",
            "11900 [D loss: 0.008589, acc.: 100.00%] [G loss: 7.789140]\n",
            "11950 [D loss: 0.049192, acc.: 98.86%] [G loss: 7.729049]\n",
            "12000 [D loss: 0.018434, acc.: 99.62%] [G loss: 5.461939]\n",
            "12050 [D loss: 0.020789, acc.: 99.62%] [G loss: 6.379803]\n",
            "12100 [D loss: 0.026911, acc.: 99.24%] [G loss: 7.305493]\n",
            "12150 [D loss: 0.001058, acc.: 100.00%] [G loss: 12.103176]\n",
            "12200 [D loss: 0.011229, acc.: 99.62%] [G loss: 7.549317]\n",
            "12250 [D loss: 0.002713, acc.: 100.00%] [G loss: 8.260489]\n",
            "12300 [D loss: 0.011362, acc.: 100.00%] [G loss: 7.719382]\n",
            "12350 [D loss: 0.012044, acc.: 99.62%] [G loss: 7.067843]\n",
            "12400 [D loss: 0.022024, acc.: 100.00%] [G loss: 6.804242]\n",
            "12450 [D loss: 0.018422, acc.: 100.00%] [G loss: 5.976219]\n",
            "12500 [D loss: 0.017370, acc.: 99.24%] [G loss: 6.233675]\n",
            "12550 [D loss: 0.008683, acc.: 100.00%] [G loss: 6.971449]\n",
            "12600 [D loss: 0.011303, acc.: 100.00%] [G loss: 7.680520]\n",
            "12650 [D loss: 0.006632, acc.: 100.00%] [G loss: 7.519252]\n",
            "12700 [D loss: 0.002755, acc.: 100.00%] [G loss: 9.646357]\n",
            "12750 [D loss: 0.006037, acc.: 100.00%] [G loss: 7.951586]\n",
            "12800 [D loss: 0.017544, acc.: 99.24%] [G loss: 11.850911]\n",
            "12850 [D loss: 0.015582, acc.: 99.62%] [G loss: 8.793688]\n",
            "12900 [D loss: 0.006388, acc.: 100.00%] [G loss: 10.496744]\n",
            "12950 [D loss: 0.019085, acc.: 99.62%] [G loss: 7.585960]\n",
            "13000 [D loss: 0.010834, acc.: 100.00%] [G loss: 5.286446]\n",
            "13050 [D loss: 0.005111, acc.: 100.00%] [G loss: 9.102741]\n",
            "13100 [D loss: 0.004123, acc.: 100.00%] [G loss: 6.959034]\n",
            "13150 [D loss: 0.008844, acc.: 99.62%] [G loss: 8.017538]\n",
            "13200 [D loss: 0.004486, acc.: 100.00%] [G loss: 7.806987]\n",
            "13250 [D loss: 0.003139, acc.: 100.00%] [G loss: 9.088595]\n",
            "13300 [D loss: 0.353321, acc.: 91.29%] [G loss: 18.690113]\n",
            "13350 [D loss: 0.032729, acc.: 98.86%] [G loss: 6.789399]\n",
            "13400 [D loss: 0.017970, acc.: 100.00%] [G loss: 6.478808]\n",
            "13450 [D loss: 0.009111, acc.: 99.62%] [G loss: 11.190266]\n",
            "13500 [D loss: 0.042658, acc.: 98.48%] [G loss: 7.991354]\n",
            "13550 [D loss: 0.005201, acc.: 100.00%] [G loss: 8.911774]\n",
            "13600 [D loss: 0.088071, acc.: 96.97%] [G loss: 7.419116]\n",
            "13650 [D loss: 0.036719, acc.: 99.24%] [G loss: 5.797298]\n",
            "13700 [D loss: 0.050365, acc.: 98.11%] [G loss: 7.804803]\n",
            "13750 [D loss: 0.014318, acc.: 100.00%] [G loss: 6.634538]\n",
            "13800 [D loss: 0.012095, acc.: 100.00%] [G loss: 6.539375]\n",
            "13850 [D loss: 0.003772, acc.: 100.00%] [G loss: 7.986618]\n",
            "13900 [D loss: 0.050238, acc.: 98.11%] [G loss: 11.704526]\n",
            "13950 [D loss: 0.064207, acc.: 98.48%] [G loss: 7.400878]\n",
            "14000 [D loss: 0.024151, acc.: 99.62%] [G loss: 5.982483]\n",
            "14050 [D loss: 0.006082, acc.: 100.00%] [G loss: 8.320714]\n",
            "14100 [D loss: 0.010971, acc.: 100.00%] [G loss: 7.679918]\n",
            "14150 [D loss: 0.009001, acc.: 100.00%] [G loss: 8.594300]\n",
            "14200 [D loss: 0.009190, acc.: 100.00%] [G loss: 10.565756]\n",
            "14250 [D loss: 0.020431, acc.: 98.48%] [G loss: 9.605803]\n",
            "14300 [D loss: 7.044988, acc.: 50.00%] [G loss: 0.000022]\n",
            "14350 [D loss: 0.052486, acc.: 97.35%] [G loss: 6.446121]\n",
            "14400 [D loss: 0.025480, acc.: 99.62%] [G loss: 7.782884]\n",
            "14450 [D loss: 0.028777, acc.: 98.86%] [G loss: 6.107162]\n",
            "14500 [D loss: 0.016983, acc.: 99.62%] [G loss: 7.381877]\n",
            "14550 [D loss: 0.036837, acc.: 99.24%] [G loss: 8.081772]\n",
            "14600 [D loss: 0.009706, acc.: 100.00%] [G loss: 8.203140]\n",
            "14650 [D loss: 0.045791, acc.: 98.86%] [G loss: 7.163987]\n",
            "14700 [D loss: 0.002993, acc.: 100.00%] [G loss: 14.863557]\n",
            "14750 [D loss: 0.042666, acc.: 99.24%] [G loss: 5.881342]\n",
            "14800 [D loss: 0.021257, acc.: 100.00%] [G loss: 6.229569]\n",
            "14850 [D loss: 0.018439, acc.: 99.62%] [G loss: 7.066019]\n",
            "14900 [D loss: 0.013613, acc.: 100.00%] [G loss: 6.574656]\n",
            "14950 [D loss: 0.024846, acc.: 99.62%] [G loss: 11.229341]\n",
            "15000 [D loss: 0.738203, acc.: 80.30%] [G loss: 30.128975]\n",
            "15050 [D loss: 0.018038, acc.: 100.00%] [G loss: 6.743677]\n",
            "15100 [D loss: 0.011631, acc.: 100.00%] [G loss: 5.928682]\n",
            "15150 [D loss: 0.066108, acc.: 98.11%] [G loss: 6.958008]\n",
            "15200 [D loss: 0.011495, acc.: 99.62%] [G loss: 7.869579]\n",
            "15250 [D loss: 0.034628, acc.: 98.48%] [G loss: 7.844295]\n",
            "15300 [D loss: 0.004859, acc.: 100.00%] [G loss: 7.859756]\n",
            "15350 [D loss: 0.020623, acc.: 99.24%] [G loss: 8.719125]\n",
            "15400 [D loss: 0.051555, acc.: 98.11%] [G loss: 7.254936]\n",
            "15450 [D loss: 0.020959, acc.: 99.62%] [G loss: 6.438026]\n",
            "15500 [D loss: 0.018208, acc.: 100.00%] [G loss: 7.237376]\n",
            "15550 [D loss: 0.009291, acc.: 100.00%] [G loss: 6.926559]\n",
            "15600 [D loss: 0.012271, acc.: 100.00%] [G loss: 8.119210]\n",
            "15650 [D loss: 0.023134, acc.: 99.62%] [G loss: 7.516932]\n",
            "15700 [D loss: 0.231048, acc.: 90.91%] [G loss: 5.989070]\n",
            "15750 [D loss: 0.033426, acc.: 99.62%] [G loss: 7.020979]\n",
            "15800 [D loss: 0.028951, acc.: 99.24%] [G loss: 6.258503]\n",
            "15850 [D loss: 0.121546, acc.: 95.45%] [G loss: 14.779161]\n",
            "15900 [D loss: 0.003321, acc.: 100.00%] [G loss: 7.736659]\n",
            "15950 [D loss: 0.018806, acc.: 100.00%] [G loss: 6.687828]\n",
            "16000 [D loss: 0.030687, acc.: 99.24%] [G loss: 6.187515]\n",
            "16050 [D loss: 0.093038, acc.: 97.35%] [G loss: 13.700560]\n",
            "16100 [D loss: 0.051403, acc.: 98.48%] [G loss: 5.723237]\n",
            "16150 [D loss: 0.026826, acc.: 99.62%] [G loss: 6.215688]\n",
            "16200 [D loss: 0.041813, acc.: 98.86%] [G loss: 15.707427]\n",
            "16250 [D loss: 0.034565, acc.: 99.62%] [G loss: 5.691348]\n",
            "16300 [D loss: 0.025869, acc.: 99.62%] [G loss: 9.416169]\n",
            "16350 [D loss: 0.020588, acc.: 100.00%] [G loss: 7.910947]\n",
            "16400 [D loss: 0.014653, acc.: 99.62%] [G loss: 6.440201]\n",
            "16450 [D loss: 0.021174, acc.: 99.62%] [G loss: 9.458958]\n",
            "16500 [D loss: 0.048043, acc.: 98.11%] [G loss: 8.369103]\n",
            "16550 [D loss: 0.027530, acc.: 99.24%] [G loss: 7.522658]\n",
            "16600 [D loss: 0.027576, acc.: 100.00%] [G loss: 7.514803]\n",
            "16650 [D loss: 0.020891, acc.: 99.62%] [G loss: 7.185539]\n",
            "16700 [D loss: 0.020255, acc.: 99.62%] [G loss: 9.673923]\n",
            "16750 [D loss: 0.080487, acc.: 98.11%] [G loss: 7.974278]\n",
            "16800 [D loss: 2.011035, acc.: 68.56%] [G loss: 53.447327]\n",
            "16850 [D loss: 0.027338, acc.: 99.24%] [G loss: 6.135966]\n",
            "16900 [D loss: 0.034839, acc.: 98.86%] [G loss: 7.848109]\n",
            "16950 [D loss: 0.026139, acc.: 99.24%] [G loss: 10.824498]\n",
            "17000 [D loss: 0.011966, acc.: 99.24%] [G loss: 7.337967]\n",
            "17050 [D loss: 0.043334, acc.: 98.86%] [G loss: 8.833626]\n",
            "17100 [D loss: 0.003466, acc.: 100.00%] [G loss: 11.161316]\n",
            "17150 [D loss: 0.026920, acc.: 99.24%] [G loss: 5.315635]\n",
            "17200 [D loss: 0.046863, acc.: 99.24%] [G loss: 9.772696]\n",
            "17250 [D loss: 0.036780, acc.: 98.11%] [G loss: 19.287766]\n",
            "17300 [D loss: 0.115756, acc.: 95.83%] [G loss: 10.417749]\n",
            "17350 [D loss: 0.042448, acc.: 99.24%] [G loss: 6.554955]\n",
            "17400 [D loss: 0.019695, acc.: 99.62%] [G loss: 6.941461]\n",
            "17450 [D loss: 0.035434, acc.: 99.24%] [G loss: 9.010895]\n",
            "17500 [D loss: 0.052245, acc.: 98.48%] [G loss: 8.510713]\n",
            "17550 [D loss: 4.986038, acc.: 50.00%] [G loss: 1.134022]\n",
            "17600 [D loss: 0.040505, acc.: 99.24%] [G loss: 3.337626]\n",
            "17650 [D loss: 0.051411, acc.: 98.48%] [G loss: 7.944362]\n",
            "17700 [D loss: 0.043225, acc.: 99.24%] [G loss: 5.540932]\n",
            "17750 [D loss: 0.023515, acc.: 99.62%] [G loss: 7.364202]\n",
            "17800 [D loss: 0.060594, acc.: 97.73%] [G loss: 8.318578]\n",
            "17850 [D loss: 0.070447, acc.: 96.97%] [G loss: 8.716232]\n",
            "17900 [D loss: 0.056305, acc.: 98.11%] [G loss: 6.508553]\n",
            "17950 [D loss: 0.029283, acc.: 98.86%] [G loss: 7.802568]\n",
            "18000 [D loss: 0.023777, acc.: 99.62%] [G loss: 8.036986]\n",
            "18050 [D loss: 0.025832, acc.: 98.86%] [G loss: 10.377362]\n",
            "18100 [D loss: 0.099215, acc.: 96.97%] [G loss: 12.545622]\n",
            "18150 [D loss: 0.060392, acc.: 98.48%] [G loss: 6.467921]\n",
            "18200 [D loss: 0.017990, acc.: 99.24%] [G loss: 9.491745]\n",
            "18250 [D loss: 0.072060, acc.: 98.11%] [G loss: 8.683548]\n",
            "18300 [D loss: 0.029177, acc.: 99.24%] [G loss: 7.831769]\n",
            "18350 [D loss: 0.017368, acc.: 100.00%] [G loss: 7.909635]\n",
            "18400 [D loss: 0.071049, acc.: 96.97%] [G loss: 9.424254]\n",
            "18450 [D loss: 0.050234, acc.: 98.11%] [G loss: 5.144864]\n",
            "18500 [D loss: 0.058013, acc.: 98.48%] [G loss: 9.753179]\n",
            "18550 [D loss: 0.210619, acc.: 89.77%] [G loss: 5.938909]\n",
            "18600 [D loss: 0.031928, acc.: 99.24%] [G loss: 7.201341]\n",
            "18650 [D loss: 0.011004, acc.: 99.62%] [G loss: 9.619907]\n",
            "18700 [D loss: 0.039055, acc.: 99.24%] [G loss: 7.853912]\n",
            "18750 [D loss: 0.087420, acc.: 96.97%] [G loss: 10.952209]\n",
            "18800 [D loss: 0.059727, acc.: 97.73%] [G loss: 9.945617]\n",
            "18850 [D loss: 0.108740, acc.: 96.59%] [G loss: 7.897736]\n",
            "18900 [D loss: 0.064658, acc.: 97.73%] [G loss: 10.070273]\n",
            "18950 [D loss: 0.070833, acc.: 97.35%] [G loss: 8.134142]\n",
            "19000 [D loss: 0.069143, acc.: 97.73%] [G loss: 9.438403]\n",
            "19050 [D loss: 0.144518, acc.: 95.08%] [G loss: 5.610030]\n",
            "19100 [D loss: 0.124271, acc.: 95.08%] [G loss: 7.325696]\n",
            "19150 [D loss: 0.152251, acc.: 93.18%] [G loss: 7.534798]\n",
            "19200 [D loss: 0.026481, acc.: 99.24%] [G loss: 10.196228]\n",
            "19250 [D loss: 0.049587, acc.: 98.11%] [G loss: 6.222061]\n",
            "19300 [D loss: 0.047057, acc.: 98.86%] [G loss: 8.789239]\n",
            "19350 [D loss: 0.117678, acc.: 95.45%] [G loss: 7.335148]\n",
            "19400 [D loss: 0.167824, acc.: 90.91%] [G loss: 8.956189]\n",
            "19450 [D loss: 0.080696, acc.: 97.35%] [G loss: 7.228168]\n",
            "19500 [D loss: 0.148677, acc.: 93.94%] [G loss: 9.695991]\n",
            "19550 [D loss: 0.098552, acc.: 96.21%] [G loss: 13.910358]\n",
            "19600 [D loss: 0.093444, acc.: 95.83%] [G loss: 7.419683]\n",
            "19650 [D loss: 0.053000, acc.: 98.11%] [G loss: 7.640630]\n",
            "19700 [D loss: 0.100549, acc.: 95.08%] [G loss: 8.192867]\n",
            "19750 [D loss: 0.120562, acc.: 95.83%] [G loss: 9.139426]\n",
            "19800 [D loss: 0.046520, acc.: 98.11%] [G loss: 6.042724]\n",
            "19850 [D loss: 0.044700, acc.: 98.11%] [G loss: 8.184509]\n",
            "19900 [D loss: 0.043155, acc.: 98.48%] [G loss: 7.088477]\n",
            "19950 [D loss: 0.076513, acc.: 96.97%] [G loss: 7.083307]\n",
            "20000 [D loss: 0.015835, acc.: 100.00%] [G loss: 7.702332]\n",
            "20050 [D loss: 0.200833, acc.: 91.29%] [G loss: 7.385334]\n",
            "20100 [D loss: 0.078659, acc.: 96.21%] [G loss: 7.985420]\n",
            "20150 [D loss: 0.082025, acc.: 96.59%] [G loss: 8.088581]\n",
            "20200 [D loss: 0.110473, acc.: 96.21%] [G loss: 12.707435]\n",
            "20250 [D loss: 0.061952, acc.: 97.73%] [G loss: 11.011944]\n",
            "20300 [D loss: 0.178144, acc.: 93.56%] [G loss: 6.640362]\n",
            "20350 [D loss: 0.142481, acc.: 94.32%] [G loss: 12.577150]\n",
            "20400 [D loss: 0.080849, acc.: 97.35%] [G loss: 9.330585]\n",
            "20450 [D loss: 0.039189, acc.: 98.86%] [G loss: 7.685028]\n",
            "20500 [D loss: 0.078575, acc.: 97.73%] [G loss: 7.611720]\n",
            "20550 [D loss: 0.152516, acc.: 94.70%] [G loss: 6.740398]\n",
            "20600 [D loss: 0.105510, acc.: 95.45%] [G loss: 9.973753]\n",
            "20650 [D loss: 0.041730, acc.: 99.24%] [G loss: 7.753270]\n",
            "20700 [D loss: 0.138095, acc.: 95.08%] [G loss: 8.303633]\n",
            "20750 [D loss: 0.064752, acc.: 98.86%] [G loss: 7.814833]\n",
            "20800 [D loss: 0.081136, acc.: 97.73%] [G loss: 8.183753]\n",
            "20850 [D loss: 0.194758, acc.: 92.05%] [G loss: 6.215670]\n",
            "20900 [D loss: 0.109035, acc.: 95.45%] [G loss: 6.279366]\n",
            "20950 [D loss: 0.025903, acc.: 99.62%] [G loss: 7.762616]\n",
            "21000 [D loss: 0.197100, acc.: 92.80%] [G loss: 8.901288]\n",
            "21050 [D loss: 0.151396, acc.: 94.32%] [G loss: 5.598948]\n",
            "21100 [D loss: 0.087306, acc.: 96.59%] [G loss: 10.113111]\n",
            "21150 [D loss: 0.057271, acc.: 98.11%] [G loss: 10.194133]\n",
            "21200 [D loss: 0.169010, acc.: 93.56%] [G loss: 7.770248]\n",
            "21250 [D loss: 0.164239, acc.: 95.08%] [G loss: 8.659519]\n",
            "21300 [D loss: 0.023222, acc.: 99.24%] [G loss: 7.646910]\n",
            "21350 [D loss: 0.136205, acc.: 95.83%] [G loss: 5.512295]\n",
            "21400 [D loss: 0.081161, acc.: 97.73%] [G loss: 6.757247]\n",
            "21450 [D loss: 0.063456, acc.: 97.73%] [G loss: 6.621604]\n",
            "21500 [D loss: 0.084197, acc.: 96.59%] [G loss: 10.596138]\n",
            "21550 [D loss: 0.102091, acc.: 95.83%] [G loss: 8.668059]\n",
            "21600 [D loss: 0.111883, acc.: 95.83%] [G loss: 9.496459]\n",
            "21650 [D loss: 0.052575, acc.: 97.73%] [G loss: 14.556276]\n",
            "21700 [D loss: 0.114164, acc.: 95.08%] [G loss: 9.283259]\n",
            "21750 [D loss: 0.124539, acc.: 96.59%] [G loss: 15.924501]\n",
            "21800 [D loss: 0.069948, acc.: 96.97%] [G loss: 8.326866]\n",
            "21850 [D loss: 0.109317, acc.: 95.83%] [G loss: 7.332543]\n",
            "21900 [D loss: 0.093430, acc.: 96.97%] [G loss: 6.441313]\n",
            "21950 [D loss: 0.070524, acc.: 98.86%] [G loss: 7.788743]\n",
            "22000 [D loss: 0.104964, acc.: 96.21%] [G loss: 10.433266]\n",
            "22050 [D loss: 0.058671, acc.: 98.86%] [G loss: 8.890972]\n",
            "22100 [D loss: 0.146626, acc.: 95.83%] [G loss: 9.705264]\n",
            "22150 [D loss: 0.173253, acc.: 96.21%] [G loss: 9.843434]\n",
            "22200 [D loss: 0.066374, acc.: 97.73%] [G loss: 6.425953]\n",
            "22250 [D loss: 0.153688, acc.: 94.32%] [G loss: 8.641982]\n",
            "22300 [D loss: 0.091158, acc.: 95.45%] [G loss: 8.173969]\n",
            "22350 [D loss: 0.109951, acc.: 95.83%] [G loss: 9.184975]\n",
            "22400 [D loss: 0.046540, acc.: 98.48%] [G loss: 7.333367]\n",
            "22450 [D loss: 0.155976, acc.: 94.32%] [G loss: 8.320036]\n",
            "22500 [D loss: 0.182684, acc.: 94.32%] [G loss: 6.710774]\n",
            "22550 [D loss: 0.101891, acc.: 96.21%] [G loss: 5.226155]\n",
            "22600 [D loss: 0.139480, acc.: 95.08%] [G loss: 7.051718]\n",
            "22650 [D loss: 0.087287, acc.: 96.59%] [G loss: 8.405714]\n",
            "22700 [D loss: 0.125046, acc.: 95.45%] [G loss: 6.506883]\n",
            "22750 [D loss: 0.051810, acc.: 97.73%] [G loss: 8.349384]\n",
            "22800 [D loss: 0.155276, acc.: 93.56%] [G loss: 6.147234]\n",
            "22850 [D loss: 0.073014, acc.: 97.73%] [G loss: 7.920069]\n",
            "22900 [D loss: 0.080976, acc.: 97.73%] [G loss: 11.288097]\n",
            "22950 [D loss: 0.114605, acc.: 95.83%] [G loss: 7.393087]\n",
            "23000 [D loss: 0.098789, acc.: 96.59%] [G loss: 8.163636]\n",
            "23050 [D loss: 0.140481, acc.: 95.08%] [G loss: 6.416338]\n",
            "23100 [D loss: 0.123594, acc.: 95.45%] [G loss: 6.446028]\n",
            "23150 [D loss: 0.090539, acc.: 96.97%] [G loss: 7.056076]\n",
            "23200 [D loss: 0.095899, acc.: 96.59%] [G loss: 7.775031]\n",
            "23250 [D loss: 0.067171, acc.: 98.11%] [G loss: 9.306158]\n",
            "23300 [D loss: 0.207085, acc.: 91.67%] [G loss: 8.992794]\n",
            "23350 [D loss: 0.161011, acc.: 93.18%] [G loss: 6.075269]\n",
            "23400 [D loss: 0.303123, acc.: 87.12%] [G loss: 3.748653]\n",
            "23450 [D loss: 0.191365, acc.: 92.05%] [G loss: 5.573766]\n",
            "23500 [D loss: 0.265354, acc.: 88.26%] [G loss: 6.458755]\n",
            "23550 [D loss: 0.043216, acc.: 97.73%] [G loss: 11.306848]\n",
            "23600 [D loss: 0.068186, acc.: 98.48%] [G loss: 9.420809]\n",
            "23650 [D loss: 0.187783, acc.: 92.05%] [G loss: 7.563012]\n",
            "23700 [D loss: 0.058674, acc.: 98.48%] [G loss: 7.777401]\n",
            "23750 [D loss: 0.349919, acc.: 87.50%] [G loss: 9.361526]\n",
            "23800 [D loss: 0.116091, acc.: 95.83%] [G loss: 9.772205]\n",
            "23850 [D loss: 0.186144, acc.: 93.56%] [G loss: 4.999712]\n",
            "23900 [D loss: 0.219411, acc.: 91.67%] [G loss: 6.501442]\n",
            "23950 [D loss: 0.133296, acc.: 93.56%] [G loss: 7.622597]\n",
            "24000 [D loss: 0.888992, acc.: 66.67%] [G loss: 4.407890]\n",
            "24050 [D loss: 0.219196, acc.: 89.02%] [G loss: 5.488829]\n",
            "24100 [D loss: 0.150908, acc.: 94.70%] [G loss: 2.987174]\n",
            "24150 [D loss: 0.108960, acc.: 96.21%] [G loss: 8.052971]\n",
            "24200 [D loss: 0.239375, acc.: 90.53%] [G loss: 8.617821]\n",
            "24250 [D loss: 0.136093, acc.: 95.08%] [G loss: 7.896173]\n",
            "24300 [D loss: 0.235459, acc.: 89.39%] [G loss: 9.584908]\n",
            "24350 [D loss: 0.091886, acc.: 96.97%] [G loss: 6.698157]\n",
            "24400 [D loss: 0.158006, acc.: 93.18%] [G loss: 7.568071]\n",
            "24450 [D loss: 0.145535, acc.: 93.94%] [G loss: 8.452163]\n",
            "24500 [D loss: 0.115373, acc.: 96.59%] [G loss: 6.327184]\n",
            "24550 [D loss: 0.192552, acc.: 92.42%] [G loss: 5.677845]\n",
            "24600 [D loss: 0.154353, acc.: 93.18%] [G loss: 5.734995]\n",
            "24650 [D loss: 0.242749, acc.: 92.80%] [G loss: 7.825895]\n",
            "24700 [D loss: 0.177694, acc.: 92.05%] [G loss: 6.606560]\n",
            "24750 [D loss: 0.252436, acc.: 90.91%] [G loss: 9.063546]\n",
            "24800 [D loss: 0.117052, acc.: 94.70%] [G loss: 9.368623]\n",
            "24850 [D loss: 0.124348, acc.: 96.21%] [G loss: 6.743209]\n",
            "24900 [D loss: 0.157088, acc.: 93.56%] [G loss: 9.909964]\n",
            "24950 [D loss: 0.370879, acc.: 88.26%] [G loss: 8.690230]\n",
            "25000 [D loss: 0.141907, acc.: 95.08%] [G loss: 7.024979]\n",
            "25050 [D loss: 0.096955, acc.: 96.59%] [G loss: 7.811902]\n",
            "25100 [D loss: 0.127382, acc.: 95.45%] [G loss: 8.500435]\n",
            "25150 [D loss: 0.161661, acc.: 93.94%] [G loss: 5.447390]\n",
            "25200 [D loss: 0.150929, acc.: 94.32%] [G loss: 7.396914]\n",
            "25250 [D loss: 0.079426, acc.: 97.35%] [G loss: 6.382884]\n",
            "25300 [D loss: 0.159146, acc.: 93.56%] [G loss: 7.963265]\n",
            "25350 [D loss: 0.077936, acc.: 96.97%] [G loss: 6.412535]\n",
            "25400 [D loss: 0.182329, acc.: 92.05%] [G loss: 6.601165]\n",
            "25450 [D loss: 0.159600, acc.: 93.18%] [G loss: 5.162429]\n",
            "25500 [D loss: 0.178482, acc.: 93.18%] [G loss: 7.962334]\n",
            "25550 [D loss: 0.128057, acc.: 95.83%] [G loss: 6.971674]\n",
            "25600 [D loss: 0.112023, acc.: 96.21%] [G loss: 6.680505]\n",
            "25650 [D loss: 0.058227, acc.: 98.11%] [G loss: 6.611605]\n",
            "25700 [D loss: 0.265624, acc.: 89.02%] [G loss: 5.530615]\n",
            "25750 [D loss: 0.357581, acc.: 87.12%] [G loss: 9.064227]\n",
            "25800 [D loss: 0.070537, acc.: 97.35%] [G loss: 4.133367]\n",
            "25850 [D loss: 0.230503, acc.: 90.53%] [G loss: 6.708502]\n",
            "25900 [D loss: 0.188910, acc.: 95.08%] [G loss: 6.862765]\n",
            "25950 [D loss: 0.147301, acc.: 93.56%] [G loss: 5.801336]\n",
            "26000 [D loss: 0.160721, acc.: 93.18%] [G loss: 6.050295]\n",
            "26050 [D loss: 0.055669, acc.: 98.11%] [G loss: 7.344259]\n",
            "26100 [D loss: 0.300412, acc.: 87.88%] [G loss: 4.548027]\n",
            "26150 [D loss: 0.417701, acc.: 83.33%] [G loss: 5.422936]\n",
            "26200 [D loss: 0.123810, acc.: 95.45%] [G loss: 6.325752]\n",
            "26250 [D loss: 0.232070, acc.: 90.15%] [G loss: 4.568053]\n",
            "26300 [D loss: 0.198414, acc.: 92.05%] [G loss: 7.958442]\n",
            "26350 [D loss: 0.204998, acc.: 92.80%] [G loss: 9.348465]\n",
            "26400 [D loss: 0.090379, acc.: 96.59%] [G loss: 7.217066]\n",
            "26450 [D loss: 0.070769, acc.: 98.11%] [G loss: 7.898199]\n",
            "26500 [D loss: 0.241809, acc.: 88.64%] [G loss: 5.515186]\n",
            "26550 [D loss: 0.190116, acc.: 92.42%] [G loss: 7.229740]\n",
            "26600 [D loss: 0.196469, acc.: 91.29%] [G loss: 11.338036]\n",
            "26650 [D loss: 0.190504, acc.: 92.80%] [G loss: 6.297334]\n",
            "26700 [D loss: 0.114413, acc.: 95.08%] [G loss: 6.462957]\n",
            "26750 [D loss: 0.212563, acc.: 90.91%] [G loss: 7.704616]\n",
            "26800 [D loss: 0.281200, acc.: 88.64%] [G loss: 8.219227]\n",
            "26850 [D loss: 0.069743, acc.: 97.73%] [G loss: 7.435627]\n",
            "26900 [D loss: 0.242836, acc.: 90.91%] [G loss: 5.917747]\n",
            "26950 [D loss: 0.110494, acc.: 94.32%] [G loss: 8.781972]\n",
            "27000 [D loss: 0.267776, acc.: 90.91%] [G loss: 4.537994]\n",
            "27050 [D loss: 0.120462, acc.: 95.83%] [G loss: 7.695014]\n",
            "27100 [D loss: 0.172546, acc.: 92.80%] [G loss: 6.266037]\n",
            "27150 [D loss: 0.137180, acc.: 95.08%] [G loss: 7.397128]\n",
            "27200 [D loss: 0.228439, acc.: 93.56%] [G loss: 7.375797]\n",
            "27250 [D loss: 0.118418, acc.: 95.08%] [G loss: 6.369751]\n",
            "27300 [D loss: 0.367597, acc.: 88.26%] [G loss: 11.011798]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bb93339f034e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3ac7ce0e97a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             '''TODO: Posteriormente de haber realizado una actualización de los pesos del generador, generaremos un batch de\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                                                     class_weight)\n\u001b[1;32m   1350\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}